text:
  noise_add_probability: 0.1
  max_noise_entities_or_structures: 0.3

deduplicate_dataset:
  ngram-size: 5
  num-perm: 256
  threshold: 0.85

accelerate:
  multi_gpu: True
  gpu_ids: 0,1,2,3
  machine_rank: 0
  main_training_function: main
  mixed_precision: no
  num_machines: 1
  num_processes: 4
  rdzv_backend: static
  same_network: True

train:
  validation_frac: 0.02
  output_dir: '../../../model/logic_flows_finetune_v2/model'
  learning_rate: 5e-5
  weight_decay: 0.05
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8
  max_steps: 600000
  warmup_steps: 100
  fp16: False
  save_total_limit: 4
  eval_steps: 3000
  save_steps: 3000
  logging_steps: 3000
  seed: 42
  data_seed: 42
  load_best_model_at_end: True
  metric_for_best_model: eval_loss
  early_stopping_patience: 3

model:
  model_type: enc-dec
  training_type: masking
  tokenizer_name: 'Salesforce/codet5-small'
  model_name: 'Salesforce/codet5-small'
  max_number_of_train_tokens: 2048
  max_gen_length: 256

test:
  model_dir: '../../../model/logic_flows_finetune_v2/model'
  verbose: 1
  batch_size: 4
  num_generate_sequences: 5
  random_seed: 42
  max_instances: 9999999
  max_new_tokens: 256

pack_assets:
  representation_type: python_v2
  model_name: codet5
  model_dir_path: ../../../model/logic_flows_finetune_v2/model
  custom_experiment_name: null  # we will use the default name of {model_name}_{representation_type}_oslm{package_version}_{short_commit_id}
  no_git: false